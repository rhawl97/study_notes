## 1.1 Transfer Learning

### **1) Intro**

- Motivations: ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ë‹¤ë¥¸ ì‚¬ë¬¼ì„ ì˜ˆì¸¡í•˜ëŠ” visionì˜ ë°©ì‹ ì°¸ê³  â†’ ë‹¤ë¥¸ ë„ë©”ì¸ì˜ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ë„ ìˆë‹¤. (ê³µí†µëœ feature ì°¾ê¸°)
- ê·¸ëŸ¼ NLPì—ì„œ ê³µí†µëœ featureëŠ” ë­˜ê¹Œ?
ì–´ë ¤ìš´ ë‹¨ì–´ë¥¼ í•™ìŠµí•  ë•Œ, ì‰¬ìš´ ë‹¨ì–´ë¥¼ ë¨¼ì € ë§ì´ í•™ìŠµí•˜ê³  ë‚œ ëª¨ë¸ì´ ë” ì˜ ì¸ì‹í•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?

### 2) Transfer Learningì´ë€?

- íŠ¹ì • í•˜ë‚˜ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ë‹¤ë¥¸ ê´€ë ¨ëœ ë¬¸ì œì—ì„œ ì–»ì€ ì§€ì‹ì„ í™œìš©í•œë‹¤.
- Big Dataset â†’ Pretrainingì—ì„œ ì–»ì€ weights load
- í•´ë‹¹ weightìœ¼ë¡œ Target Datasetì— ì ìš©í•œë‹¤: Fine-tuning
- ì›ë˜ëŠ” randomí•˜ê²Œ initializeëœ weightë¡œ target datasetì— ì ìš©í–ˆì§€ë§Œ, ì´ datasetì˜ ì–‘ì´ ì ì„ ê²½ìš° pretrainedëœ weightì„ í™œìš©!

### 3) How to

(1) Set seed weights and train as normal 

: í‰ì†Œì²˜ëŸ¼ í•™ìŠµ

(2) Fix loaded weights and train unloaded parts

: pretrained weights(freeze) + ë‹¬ë¼ì§„ class ê°œìˆ˜ì— í•´ë‹¹í•˜ëŠ” initialized weights(ex. softmax layer)

(3) Train with different learning rate on each part

: freezeëœ pretrained weightsëŠ” ë‚®ì€ ì†ë„ë¡œ í•™ìŠµ, ìƒˆë¡œ initializeí•œ softmax layerì™€ ê°™ì€ ì‹ ê·œ weightëŠ” ë¹ ë¥¸ ì†ë„ë¡œ í•™ìŠµí•˜ë„ë¡ ë‹¤ë¥¸ learning rate ì£¼ê¸°

### 4) Wrap-up

- ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ë“¤ì€ ì£¼ìš” íŠ¹ì§•ì„ ê³µìœ í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤.
ex. word sense, syntax(NLP), edge, border, ê³¡ë©´, ì§ì„  ë“±(vision)
- í° ë°ì´í„°ì…‹ì„ í†µí•´ ë¯¸ë¦¬ í›ˆë ¨í•œ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‚´ taskì— fine-tuningì‹œí‚¤ì!

## 1.2 Self-supervised Learning

### **1) Self-supervised Learning**

- Unsupervised Learning
    
    : ì¸ê°„ì— ì˜í•œ labelì—†ì´, ë°ì´í„° x ìì²´ì˜ ë‚´ë¶€ í‘œí˜„ ë˜ëŠ” featureë¥¼ í•™ìŠµ
    
    â†’ GAN ë“± Generative Modeling, AutoEncoder ë“± Representation Learning(ë²¡í„°í™”)
    
- Self-supervised Learning
    
    : ë°ì´í„° ë‚´ë¶€ êµ¬ì¡°ë¥¼ í™œìš©í•´ì„œ labelì´ ìˆëŠ” ê²ƒì²˜ëŸ¼ í•™ìŠµ
    
    â†’ ìƒ˜í”Œì˜ ì¼ë¶€ì •ë³´ëŠ” x / ë‚˜ë¨¸ì§€ ì •ë³´ëŠ” y(label) ì‚¼ì•„ì„œ ì˜ˆì¸¡
    
    â†’ ì´ë ‡ê²Œ í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ë¥¸ taskì— transfer learningí•´ì„œ ì§€ë„í•™ìŠµ ì„±ëŠ¥ ê·¹ëŒ€í™”
    
    ex. ë‹¤ë¥¸ í† í°ë“¤ í•™ìŠµì„ í†µí•´ ë¹ˆì¹¸ì— ë“±ì¥í•  í† í° ì˜ˆì¸¡
    
    â†’ sslì„ í†µí•´ ì¢‹ì€ weight paramì˜ seedë¥¼ ì–»ì–´, transfer learningì„ í†µí•´ í•œì •ëœ ë°ì´í„°ì…‹ì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ì!
    
- Contrastive Learning
    
    : íƒ€ê²Ÿ ì´ë¯¸ì§€ì™€ ë¹„ìŠ·í•œ x â†” ë¹„ìŠ·í•˜ì§€ ì•Šì€ x  ê°ê°ì˜ ê±°ë¦¬ ì°¨ì´ê°€ ì»¤ì§€ë„ë¡ í•™ìŠµ
    

## 1.3 Introduction to Pretrained Language Models

### **1) PLM**

- Transformer: Attentionë§Œì„ í™œìš©í•˜ì—¬ ì•„í‚¤í…ì²˜ êµ¬ì„±
- PLMì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ
    
    <aside>
    ğŸ§ (1) Feature-based Approach: ë” ì¢‹ì€ embeddingì„ ê°–ê²Œ í•˜ì
    (2) Fine-tuning Approach: ë” ì¢‹ì€ weight parameter seedë¥¼ ê°–ê²Œ í•˜ì
    (3) Meta-learning Approach(GPT-3): í° ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•  í•„ìš” ì—†ì´, context learningìœ¼ë¡œ í•™ìŠµ ë° ì¶”ë¡ ì„ ìˆ˜í–‰
    
    </aside>
    
- Era of PLMs
    
    ![Untitled](../04_BERT_GPT3_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/images/4-1.png)
    
    parameterê°€ linearí•˜ê²Œ ê¾¸ì¤€íˆ ìƒìŠ¹ + BERTê°€ ì½”ë“œ íš¨ìœ¨ë¡œ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ë¶ˆëŸ¬ì˜´
    

### 2**) Conclusion**

- íƒ€ê²Ÿ íƒœìŠ¤í¬ì— ë”°ë¼ transformerëª¨ë¸ì˜ ì¼ë¶€ë¥¼ ì„ íƒ(ex. Encoder or Decoder)
- ë§ì€ unlabeled corpusë¥¼ í†µí•´ general representation(embedding)í•™ìŠµ
- ì´í›„ì— target taskì— fine-tuning

## 1.4 Downstream Tasks

### 1**) Benchmark Tests**

(1) GLUE: General Language Understanding Evaluation
ex. texutal entailment, text comparison, qa, ê°ì •ë¶„ì„ ë“±

- MNLI: ì²«ë²ˆì§¸ ë¬¸ì¥ê³¼ ë‘ë²ˆì§¸ ë¬¸ì¥ ê°„ì˜ ê´€ê³„ classify
metric
- RTE(binary ë¶„ë¥˜)
- QQP: ì£¼ì–´ì§„ ë‘ ë¬¸ì¥ì´ ì‹¤ì œë¡œ semanticí•˜ê²Œ ìœ ì‚¬í•œì§€
- STS-B: ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œê°€ë¥¼ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ íŒë³„
- QNLI:  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ ì˜ ì´ë£¨ì–´ì¡ŒëŠ”ê°€
- SST-2: binary ë¶„ë¥˜(ê°ì •ë¶„ì„)
- CoLA: linguistically acceptable or not

(2) SQUAD 1.1&2.0(Stanford Question Answering Dataset)

â†’ ë²¤ì¹˜ë§ˆíŠ¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ í†µí•´ ë¬¸ì œí•´ê²°ëŠ¥ë ¥ì„ ê°€ëŠ í•˜ê±°ë‚˜ PLM ì„±ëŠ¥ì„ ì²´í¬

### 2) How to test

- Hugginface ë˜ëŠ” ParlAI ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í…ŒìŠ¤íŠ¸ì½”ë“œ ì œê³µ

```python
from datasets import load_dataset
dataset = load_dataset('squad', split = 'train)
print(', '.join(dataset for dataset in datasets_list))
```

### 3) Korean Benchmark Test

- [Naver sentiment movies corpus v1.0(NSM)](http://github.com/e9t/nsmc)
- [KLUE: Korean Language Understanding Evaluation](http://klue-benchmark.com/tasks)

### 4) Conclusion

- ê¸°ê³„ë²ˆì—­ì´ ì •ë³µëœ í›„ì— nlpì˜ ë‹¤ë¥¸ ë¶„ì•¼ê°€ ì—°êµ¬ë¨ì— ë”°ë¼ PLMì˜ ì‹œëŒ€ì— ì•ì„œ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ê°€ êµ¬ì¶•ë¨