## 4.1 NPLM

**1) 모델 기본 구조**

- NPLM(Neural Probabilistic Language Model)
    
    '단어가 어떤 순서로 쓰였는가'를 기반으로 한 통계적 언어 모델의 한계를 극복
    
    → 학습 데이터에 존재하지 않는 n-gram은 확률 값 0 부여
    
    → 장기 의존성 파악의 어려움: n이 커질수록 등장 확률이 0인 단어가 늘어남
    
    → 단어/문장 간 유사도 계산 불가
    

**2) NPLM의 학습**

- n-gram 언어 모델: 직전까지 등장한 n-1개 단어들로 다음 단어를 예측하며 학습
- NPLM 출력
    
    특정 차원의 스코어 벡터 y에 소프트맥스 함수를 적용한 확률 벡터 → 확률이 가장 높은 벡터 = 실제 정답 단어 가 되도록 학습!
    
- NPLM 입력
    
    (어휘 집합 크기) X (차원 수) 크기를 갖는 행렬에서, 입력 단어에 해당하는 벡터를 lookup한 형태
    
- 입력층, hidden layer, 출력층을 통해 이전에 등장한 n-1개 단어들로 정답 단어를 예측하며 학습 → 최종 y벡터는 softmax로!

**3) NPLM과 의미 정보**

- NPLM 원리에 담긴 의미 정보
    
    직전 n-1개 단어를 가지고 다음에 등장할 단어를 예측
    
    결국, 이전 n-1개 단어는 다음에 등장할 단어와 특정 관계가 형성됨 
    
    → 정답을 맞추는 과정에서 발생한 loss를 최소화하는 gradient를 받아 이전 단어 위치 행에 해당하는 벡터가 동일하게 업데이트됨
    
    → 결국 동일한 정답 단어를 예측하는 이전 단어들은 각 벡터가 동일한 방향의 공간으로 업데이트!
    
- 통계 기반 n-gram 모델과의 차이점
    
    한 번도 등장하지 않은 단어에 대해서 확률 0 부여 x → 비슷한 다른 문장을 참고해 확률 부여
    

---

## 4.2 Word2Vec

**1) 모델 기본 구조**

- CBOW: 주변 문맥 단어를 통해 target word를 예측하며 학습
- Skip-gram: target word를 통해 주변 문맥 단어를 예측하며 학습
    
    ⇒ Skip-Gram의 성능이 더 좋음! 더 어려운 task를 해결하며 loss를 최소화시키기 때문에
    

**2) 학습 데이터 구축**

- positive sample: target word와 실제로 등장한 context word 쌍
- negative sample: target word와 그 주변에 등장하지 않은 단어 쌍 (corpus 전체에서 랜덤 추출)
- 전체 corpus를 단어별로 슬라이딩해가며 학습 데이터 구축

<aside>
❓ 네거티브 샘플링 기법: 1개의 positive sample과  k개의 네거티브 샘플만 계산하면 되기 때문에, sigmoid를 통해 1개 단계에서 전체 단어를 모두 계산할 수 있음

</aside>

**3) 모델 학습**

- 단어 쌍이 positive인지 negative인지를 예측하는 과정으로 학습

---

## 4.3 FastText

**1) 모델 기본 구조**

- n-gram 벡터의 합으로 단어를 표현
- 네거티브 샘플링 기법 사용 → target과 context corpus를 학습할 때, 타겟 단어에 속한 n-gram 벡터들을 모두 업데이트
- 조사나 어미가 발달한 한국어에 좋은 성능을 냄 + 오타나 미등록 단어에도 좋은 성능!

**2) 한글 자소와 FastText**

- 문자 단위 n-gram ⇒ 한글 자소와 일맥상통

---

## 4.4 잠재 의미 분석

**1) PPMI 행렬**

- PMI: 두 확률변수 사이의 상관성을 계량화한 지표 → 두 단어의 등장 관계가 독립 대비 동시 등장 비율
- PPMI: PMI의 양의 점별 상호 정보량 → PMI가 양수가 아닐 경우 등장 확률이 희박해 0으로 취급
- Shift PMI = PMI - logk

**2) 행렬 분해로 이해하는 잠재 의미 분석**

- 특이값 분해: (m, n) = (m, m) x (m, n) x (n, n)
- m개 단어, n개 문서로 이루어진 단어-문서 행렬에서 각각이 단어 임베딩과 문서 임베딩에 대응함

---

## 4.5 GloVe

**1) 모델 기본 구조**

- 목적함수: 임베딩된 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 등장 빈도의 로그 값이 되도록!
    
    ⇒ Word2Vec의 단점: 말뭉치 전체의 통계 정보는 반영하기 어려움 + LSA의 단점: 결과물로 단어의 유사도를 측정하기는 어려움
    
- 학습과정: corpus로 단어-문서 행렬 만들기 → 목적함수를 최소화하는 임베딩 벡터 찾기(행렬 분해) → 행렬 분해된 U, V를 업데이트하면서 학습

---

## 4.6 Swivel

**1) 모델 기본 구조**

- Swivel: GloVe와 마찬가지로 행렬 분해 기반 단어 임베딩 기법
- GloVe는 단어-문서 행렬을 분해하지만, Swivel은 PMI행렬을 분해(타겟단어 - 문맥단어 행렬)

---

## 4.8 가중 임베딩

**1) 모델 개요**

-