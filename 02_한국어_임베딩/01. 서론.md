## 1.1 임베딩이란

**1) 임베딩이란**

- 임베딩(Embedding): 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 vector로 바꾼 결과
- 가장 단순한 임베딩은 단어의 빈도를 그대로 벡터로 사용하는 방법 → 문서 당 단어 빈도를 표로 정리한 단어-문서 행렬 이용

---

## 1.2 임베딩의 역할

**1) 단어/문장 간 관련도 계산**

- 단어 간 유사도를 비교해 나열할 수 있음   ex. 100차원 임베딩 → 코사인 유사도

**2) 의미/문법 정보 함축**

- 단어 유추 평가: 단어 간 임베딩의 덧셈, 뺄셈 등 사칙연산을 통해 단어 임베딩의 품질을 평가하는 방법   ex. 아들 - 딸 + 소녀 = 소년

**3) 전이 학습(transfer learning)**

- 전이 학습
    
    : 특정 문제를 풀기 위해 학습한 모델을 다른 문제를 푸는 데 재사용하는 기법
    
    임베딩을 다른 딥러닝 모델의 입력값으로 쓰는 기법 = 전이 학습
    
    (실험) 입력 벡터를 Random 초기화한 후 모델링 vs FastText 임베딩 벡터를 입력값으로 설정한 후 모델링
    
    ⇒ 후자의 경우가 처음부터 높은 정확도와 낮은 loss값을 보임
    

---

## 1.3 임베딩 기법의 역사와 종류

**1) 통계 기반에서 뉴럴 네트워크 기반으로**

- 잠재 의미 분석(Latent Semantic Analysis)
    
    단어 빈도 등 corpus의 통계량 정보가 들어있는 행렬에 특이값 분해 등 수학적 기법을 이용해 차원 축소시키는 방법
    
    ex. TF-IDF 행렬, Word-Context 행렬, 점별 상호 정보량 행렬
    
- Neural Network 기반 임베딩 기법
    
    이전 단어들이 주어졌을 때 다음 단어를 예측하거나, 문장 내 일부분에 masking 후 맞추는 과정으로 학습
    
    이전 잠재 의미 분석에 비해 유연한 구조 + 풍부한 표현력
    

**2) 단어 수준에서 문장 수준으로**

- 단어 수준 모델
    
    각 벡터에 단어의 문맥적 의미를 함축 ex. NPLM, Word2Vec, FastText 등
    
    But, 동음이의어를 분간하기 어렵다는 단점을 가짐 → 단어의 형태가 같다면 동일한 단어로 취급하기 때문
    
- 문장 수준 모델
    
    개별 단어가 아닌 단어 sequence 전체의 문맥적 의미를 함축 ex. ELMo, BERT, GPT 등
    
    단어 임베딩 기법보다 전이 학습 효과가 좋음
    
    동음이의어를 구별해 문맥에 따라 다르게 해석할 수 있음
    

**3) 룰 → 엔드투엔드 → 프리트레인/파인 튜닝**

- 룰(1990년대)
    
    모델에 입력값으로 넣기 위해 사람이 feature를 직접 뽑음 → 한국어에서 명사 앞에 관형사가 온다, 조사가 명사 뒤에 온다 등을 규칙으로 정의
    
- 엔드투엔드(2000년대 중반 이후)
    
    input과 output의 관계를 정의해주지 않아도 모델 스스로 처음부터 끝까지 이해하도록 유도 ex. seq2seq
    
- 프리트레인/파인 튜닝(2018~)
    
    프리트레인: 대규모 corpus를 이용해 의미적, 문법적 맥락이 포함된 임베딩을 만듦!
    
    파인 튜닝: 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만들고, 태스크에 맞게 임베딩을 포함한 모델 전체를 업데이트! 
    


<aside>
❓ 다운스트림 태스크: 우리가 풀고 싶은 자연어 처리의 구체적 문제들   ex.  품사 판별, 개체명 인식 등
업스트림 태스크: 다운스트림 태스크에 앞서 해결해야 할 과제     ex. 단어/문장 임베딩

</aside>





**4) 임베딩의 종류와 성능**

- 행렬 분해 기반 방법
    
    말뭉치 정보가 들어있는 기존 행렬을 두 개 이상의 행렬로 쪼갬 → 이 행렬들을 더하거나 이어 붙여 임베딩으로 사용  ex. GloVe, Swivel
    
- 예측 기반 방법
    
    masking된 단어를 예측하면서 학습해 단어의 임베딩을 구성하는 방법(뉴럴넷 기반)  ex. Word2Vec, FastText
    
- 토픽 기반 방법
    
    주어진 문서에 잠재된 주제를 추론하는 방식 → 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환  ex. LDA
    
- 임베딩 성능 평가
    
    파인 튜닝 모델의 구조를 고정한 뒤, 각각의 임베딩을 전이 학습시키는 형태로 평가 → 임베딩 품질이 각종 다운스트림 태스크에 큰 영향을 줌