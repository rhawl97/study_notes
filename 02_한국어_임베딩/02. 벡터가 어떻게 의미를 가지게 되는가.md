## 2.1 자연어 계산과 이해

**1) 임베딩**

- 임베딩: 컴퓨터가 자연어를 이해할 수 있도록 벡터로 바꾸는 기술
- 임베딩을 만드는 세 가지 철학
    
    <aside>
    ❓ 백오브워즈 가정: '어떤 단어가 많이 쓰였는가'    ex. TF-IDF, Deep Averaging Network
    
    </aside>
    
    ⇒ 저자의 의도는 단어 사용 여부나 그 빈도에서 드러난다는 가정 + 단어의 순서 정보는 무시
    
    <aside>
    ❓ 언어 모델: '단어가 어떤 순서로 쓰였는가'    ex. ELMo, GPT
    
    </aside>
    
    ⇒ 단어의 등장 순서를 학습 + 주어진 단어 시퀀스가 얼마나 자연스러운지
    
    <aside>
    ❓ 분포 가정: '어떤 단어가 같이 쓰였는가'    ex. PMI, Word2Vec
    
    </aside>
    
    ⇒ 단어의 의미는 그 주변 문맥을 통해 유추할 수 있다는 가정
    

---

## 2.2 어떤 단어가 많이 쓰였는가

**1) 백오브워즈 가정**

- bag of words: 문장을 단어 단위로 나눈 결과를 중복집합에 넣어 임베딩으로 활용
- 빈도를 그대로 사용한다면 단어가 주제와 더 강한 관련을 맺을 것이라는 전제
- 정보 검색 분야에 많이 이용됨 → 사용자의 질의에 적절한 문서를 보여줄 경우

**2) TF-IDF**

- 단어 빈도 임베딩의 문제점: 자주 등장한다고 해서 과연 그 주제를 대표할 수 있을까?
- 이러한 단점을 보완해 등장한 TF-IDF!

$$
TF-IDF(w) = TF(w)  *log({N \over DF(w)})
$$

<aside>
❓ TF: 어떤 단어가 특정 문서에 얼마나 많이 쓰였는지의 빈도
DF: 특정 단어가 나타난 문서의 수
IDF: 전체 문서 수를 해당 단어의 DF로 나눈 뒤 로그를 취한 값
⇒ TF가 클수록 문서에서 중요한 단어, DF가 클수록 다수 문서에 쓰이는 범용적인 단어, IDF가 클수록 특이한 단어

</aside>

- TF-IDF는 어떤 단어의 주제 예측 능력이 강할수록 가중치가 커짐

**3) Deep Averaging Network**

- 백오브워즈 가정 기반 뉴럴 네트워크 → 단어의 순서 정보를 고려하지 않는다는 점!
- 중복집합에 속한 단어들의 임베딩 mean값으로 문장 임베딩
- 문서가 어떤 범주인지 분류하는 데 많이 쓰임

---

## 2.3 단어가 어떤 순서로 쓰였는가

**1) 통계 기반 언어 모델**

- 언어 모델
    
    : 단어 시퀀스에 확률을 부여하는 모델
    
    단어 등장 순서 정보를 학습한다는 점에서 백오브워즈와 반대
    
    단어가 n개 주어진 상황에서 n개 단어가 동시에 나타날 확률을 반환 → 어떤 문장이 그럴듯한지에 대한 확률 부여
    
    ex.  누명을 쓰다: 0.41 / 누명을 당하다: 0.02
    
- 최대우도추정법
    
    : 특정 문자열 시퀀스 다음에 어떤 단어가 나타날 확률
    
    But, 한 문장이 완벽히 등장하지 않았다면 이상한 문장이 아님에도 불구하고 확률 0을 부여해버림 → n-gram 모델을 통해 해결
    
    마코프 가정(직전 n-1개 단어의 등장확률로 단어 시퀀스 등장 확률을 근사)에 기반
    
    ex. '내' 등장 확률 * '내' '마음' 등장 확률 * '내' '마음' '속에' 등장 확률 …
    
    But, 한 번도 등장하지 않은 n-gram에 대해서는 확률 0 부여 → back-off, smoothing 등장
    
- back-off
    
    n-gram 등장 빈도를 n보다 작은 빈도수로 근사함 → 7-gram 빈도를 더 작은 수로 근사하면 실제 빈도와의 차이를 보정하면서, 7-gram이 완전히 등장했다면 count 1
    
- smoothing
    
    단어 시퀀스 빈도에 모두 k를 더함 → k=1일 경우, 라플라스 스무딩이라고 칭함
    
    높은 빈도를 가진 시퀀스의 확률을 낮추고, 전혀 등장하지 않는 케이스들에 대해서 작게나마 확률 부여
    

**2) 뉴럴 네트워크 기반 언어 모델**

- NN 기반 모델
    
    주어진 단어 시퀀스를 가지고 다음 단어를 맞추는 과정으로 학습 → 모델의 중간 or 마지막 계산 결과물이 임베딩 값   ex. ELMo, GPT
    
- 마스크 언어 모델
    
    문장 중간에 마스크를 씌워 놓고 단어를 예측하는 기법 → bi-directional       ex. BERT
    
    기존 언어 모델은 주어진 단어 시퀀스를 통한 일방향 기법이기 때문에 차이가 있음
    

---

## 2.3 단어가 어떤 순서로 쓰였는가

**1) 분포 가정**

- 가정: 특정 범위(윈도우) 내에 동시에 등장하는 단어가 곧 단어의 의미를 반영
- But, 분포 정보가 단어의 의미를 완벽히 내포하기는 논리적으로 어려움

**2) 분포와 의미 (1): 형태소**

- 형태소: 의미를 가지는 최소 단위
- 해당 단어를 다른 단어가 대체할 수 있는가를 분석해 형태소 자격 부여

**3) 분포와 의미 (2): 품사**

- 품사: 단어를 문법적 성질의 공통성에 따라 묶어 놓은 것
- 품사 구분 기준: 기능, 의미, 형식 → 의미와 형식의 경우 결정적 기준으로 보기에 애매함 → 기능이 가장 결정적인 분류 기준!
- 형태소의 경계를 구분하거나 품사를 나누는 것 등의 언어학적 문제는 분포 정보와 매우 관련있음 → 임베딩에 분포 정보를 함축시킴으로써 단어의 의미를 내재시킨다!

**4) 점별 상호 정보량**

- PMI: 분포 가정에 따른 단어 가중치 할당 기법 → 단어 A와 B가 같이 등장하면 값이 커짐 ⇒ 두 단어가 독립일 때에 비해 얼마나 자주 같이 등장하는지를 수치화한 확률값

**5)  Word2Vec**

- CBOW: 문맥 단어들로 타겟 단어를 예측하며 학습 ↔ Skip-gram: 타겟 단어로 문맥 단어를 예측하며 학습